# street art auction

#search by ID's +2/-2
#save URLs into csv
#break URLs into Quarters / Months
#exception for 404s

from bs4 import BeautifulSoup
import requests
import pandas as pd
from os import path
import re
from datetime import timedelta, date
import concurrent.futures

# url = 'https://www.catawiki.com/a/'
# a = 353831
# a = 	
# a = 365691
# a = 365681
# d = []
# iterator = url.split('/')[-1:]

# HOW DO I ITERATE THROUGH 37000 AND BELOW BY 2 FOR URLS LIST?
urls = ['http://www.catawiki.com/a/3{}'.format(x) for x in range(53501,67001,2)]

CONNECTIONS = 100
TIMEOUT = 5

file_name = 'test2.csv'
out = []

def load_url(url, timeout):
	r2 = requests.head(fullurl, timeout=timeout)
	return r2.status_code

with concurrent.futures.ThreadPoolExecutor(max_workers=CONNECTIONS) as executor:
	future_to_url = (executor.submit(load_url, fullurl, TIMEOUT) for a in range(1000))

	time1 = time.time()
	for future in concurrent.futures.as_completed(future_to_url):
		try:
			data = future.result()
		except Exception as exc:
			data = str(type(exc))
		finally:
			out.append(data)
		
		print(str(len(out)),end='\r')
	time2 = time.time()

print(f'Took {time2-time1:.2f} s')

#HOW DO I WRITE DATA TO FILE FROM URLs THAT DO INTEREST ME
#MAYBE RUN CONCURRENT FUTURES TO GET 200 STATUS CODES IN A SERIES OR LIST AND THEN RUN A SEPARATE PART FOR HTML REQUESTS (DATE, URL, CATEGORY)

# for i in range(1000):

# 	fullurl	= url + str(a)
# 	r = requests.get(fullurl)
# 	stat = r.status_code
# 	soup = BeautifulSoup(r.content, 'html.parser')

# 	try:
# 		category = soup.select('h1.c-page__heading')[0].text.strip()
# 		if re.search(r'Street Art Auction', category):
# 			print(stat)
# 			print(a)
# 			datetime = soup.select('div.be-auction__header-bottom')
# 			check = re.search(r'(\d{4}).(\d{1,2}).(\d{1,2})', str(datetime))
# 			date = check.group()
# 			d.append([date, category, stat, fullurl, a])
# 	except IndexError:	
# 		b+=1
# 	except ConnectionError:
# 		print('Connectioned failed')

# 	a-=2
# 	i+=1



# if path.exists(file_name) is False:
# 	df = pd.DataFrame(d, columns=('Date', 'Category', 'Status_code', 'Full_URL', 'Path'))
# 	df.to_csv(file_name, index=False)
# else:
# 	df = pd.DataFrame(d)
# 	df.to_csv(file_name, mode='a', index=False)

# print('\nFinished writing to file!')


# a = date.today()
# today = a.strftime("%Y%m%d")
# b = today + timedelta(days=14)
# two_weeks = b.strftime("%Y%m%d")

# https://stackoverflow.com/questions/2632520/what-is-the-fastest-way-to-send-100-000-http-requests-in-python#2635066




#make a dictionary to append column-wise
list1 = []
list2 = []

dic = {'a': [], 'b': []}
dic['a'].append(list1)
dic['b'].append(list2)





# for urls
def load_url(url, timeout):
    #ans = requests.head(url, timeout=timeout)
    #return ans.status_code
    with requests.get(url, timeout=timeout) as conn:
        return conn.read()

with concurrent.futures.ThreadPoolExecutor(max_workers=CONNECTIONS) as executor:
    future_to_url = {executor.submit(load_url, url, TIMEOUT): url for url in urls}
    time1 = time.time()
    for future in concurrent.futures.as_completed(future_to_url):
        url = future_to_url[future]
        try:
            data = future.result()
        except Exception as exc:
            data = str(type(exc))
        finally:
            out.append([url])

            print(str(len(out)),end="\r")
    time2 = time.time()